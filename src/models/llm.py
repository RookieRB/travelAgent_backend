# src/models/llm.py
import os
from typing import Optional
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI

def create_llm(
    provider: str = None,
    model: str = None,
    temperature: float = 0.7,
    base_url: str = None
):
    """
    åˆ›å»º LLM å®ä¾‹
    
    Args:
        provider: æä¾›å•† (ollama/openai)ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
        model: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        base_url: API åœ°å€
        
    Returns:
        LLM å®ä¾‹
    """
    provider = provider or os.getenv("LLM_PROVIDER", "openai")
    print(f"ğŸ“Œ åˆ›å»º LLM å®ä¾‹ï¼Œæä¾›å•†: {provider}")
    if provider.lower() == "ollama":
        return create_ollama_llm(model, temperature, base_url)
    elif provider.lower() == "openai":
        return create_openai_llm(model, temperature, base_url)
    else:
        raise ValueError(f"Unsupported LLM provider: {provider}")


def create_ollama_llm(
    model: str = None,
    temperature: float = 0.7,
    base_url: str = None
) -> ChatOllama:
    """
    åˆ›å»º Ollama LLM å®ä¾‹
    
    Args:
        model: æ¨¡å‹åç§°ï¼Œå¦‚ llama3, qwen2, mistral ç­‰
        temperature: æ¸©åº¦å‚æ•°
        base_url: Ollama æœåŠ¡åœ°å€
    """
    model = model or os.getenv("OLLAMA_MODEL", "qwen2:7b")
    base_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
    
    print(f"ğŸ¦™ ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹: {model}")
    print(f"   åœ°å€: {base_url}")
    
    return ChatOllama(
        model=model,
        temperature=temperature,
        base_url=base_url,
        # Ollama ç‰¹æœ‰å‚æ•°
        num_ctx=4096,           # ä¸Šä¸‹æ–‡é•¿åº¦
        num_predict=2048,       # æœ€å¤§ç”Ÿæˆ token æ•°
        repeat_penalty=1.1,     # é‡å¤æƒ©ç½š
        top_k=40,
        top_p=0.9,
        # è¶…æ—¶è®¾ç½®
        timeout=120,            # Ollama æœ¬åœ°æ¨ç†å¯èƒ½è¾ƒæ…¢
    )


def create_openai_llm(
    model: str = None,
    temperature: float = 0.7,
    base_url: str = None
) -> ChatOpenAI:
    """åˆ›å»º OpenAI LLM å®ä¾‹"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âš ï¸ Warning: OPENAI_API_KEY not found")
        api_key = "sk-dummy-key"
    
    model = model or os.getenv("OPENAI_MODEL", "gpt-4")
    base_url = base_url or os.getenv("OPENAI_API_BASE")
    
    return ChatOpenAI(
        model=model,
        temperature=temperature,
        api_key=api_key,
        base_url=base_url,
        timeout=60.0,
        max_retries=3
    )


# ===================== å…¨å±€ LLM å®ä¾‹ =====================

Myllm = create_llm()