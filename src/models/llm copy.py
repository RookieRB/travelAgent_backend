# src/models/llm.py
import os
from typing import Optional, Literal
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.language_models.chat_models import BaseChatModel


class LLMFactory:
    """
    LLM å·¥å‚ - æ”¯æŒå¤šæä¾›å•† + æ¨¡å‹åˆ†å±‚
    
    ä½¿ç”¨æ–¹å¼:
        # è·å–è½»é‡æ¨¡å‹ï¼ˆæ‘˜è¦ã€æ ¼å¼åŒ–ï¼‰
        llm = LLMFactory.get_light_model()
        
        # è·å–æ™ºèƒ½æ¨¡å‹ï¼ˆè§„åˆ’ã€åˆ›æ„ï¼‰
        llm = LLMFactory.get_smart_model()
        
        # è·å–é»˜è®¤æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰
        llm = LLMFactory.get_default()
    """
    
    _instances: dict = {}
    
    # ===================== é…ç½® =====================
    
    @classmethod
    def _get_provider(cls) -> str:
        """è·å– LLM æä¾›å•†"""
        return os.getenv("LLM_PROVIDER", "openai").lower()
    
    @classmethod
    def _get_config(cls, model_type: Literal["light", "smart", "default"]) -> dict:
        """
        è·å–æ¨¡å‹é…ç½®
        
        ç¯å¢ƒå˜é‡ä¼˜å…ˆçº§ï¼š
        - LLM_LIGHT_MODEL: è½»é‡æ¨¡å‹åç§°
        - LLM_SMART_MODEL: æ™ºèƒ½æ¨¡å‹åç§°
        - LLM_MODEL / OPENAI_MODEL / OLLAMA_MODEL: é»˜è®¤æ¨¡å‹
        """
        provider = cls._get_provider()
        
        # æ¨¡å‹åç§°é…ç½®
        model_configs = {
            "openai": {
                "light": os.getenv("LLM_LIGHT_MODEL", "qwen-turbo"),
                "smart": os.getenv("LLM_SMART_MODEL", "qwen-long-latest"),
                "default": os.getenv("LLM_MODEL", os.getenv("OPENAI_MODEL", "qwen-long-latest")),
            },
            "ollama": {
                "light": os.getenv("LLM_LIGHT_MODEL", "qwen2:7b"),
                "smart": os.getenv("LLM_SMART_MODEL", "qwen2:14b"),
                "default": os.getenv("LLM_MODEL", os.getenv("OLLAMA_MODEL", "qwen2:7b")),
            }
        }
        
        # æ¸©åº¦é…ç½®
        temperature_configs = {
            "light": 0.3,   # è½»é‡æ¨¡å‹æ›´ç¡®å®šæ€§
            "smart": 0.7,   # æ™ºèƒ½æ¨¡å‹æ›´æœ‰åˆ›æ„
            "default": 0.7,
        }
        
        return {
            "provider": provider,
            "model": model_configs.get(provider, model_configs["openai"])[model_type],
            "temperature": float(os.getenv("LLM_TEMPERATURE", temperature_configs[model_type])),
        }
    
    # ===================== åˆ›å»ºå®ä¾‹ =====================
    
    @classmethod
    def _create_openai(cls, model: str, temperature: float) -> ChatOpenAI:
        """åˆ›å»º OpenAI å…¼å®¹çš„ LLM"""
        api_key = os.getenv("OPENAI_API_KEY","sk-34752fb47e9b4a6dac314b0feb64e13e")
        base_url = os.getenv("OPENAI_API_BASE","https://dashscope.aliyuncs.com/compatible-mode/v1")
        
        if not api_key:
            print("âš ï¸ Warning: OPENAI_API_KEY not found")
            api_key = "sk-dummy-key"
        
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            api_key=api_key,
            base_url=base_url,
            timeout=60.0,
            max_retries=3,
        )
    
    @classmethod
    def _create_ollama(cls, model: str, temperature: float) -> ChatOllama:
        """åˆ›å»º Ollama æœ¬åœ° LLM"""
        base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        
        return ChatOllama(
            model=model,
            temperature=temperature,
            base_url=base_url,
            num_ctx=4096,
            num_predict=2048,
            repeat_penalty=1.1,
            top_k=40,
            top_p=0.9,
            timeout=120,
        )
    
    @classmethod
    def _create_instance(cls, model_type: Literal["light", "smart", "default"]) -> BaseChatModel:
        """åˆ›å»º LLM å®ä¾‹"""
        config = cls._get_config(model_type)
        provider = config["provider"]
        model = config["model"]
        temperature = config["temperature"]
        
        print(f"ğŸ¤– åˆ›å»º {model_type} æ¨¡å‹: {provider}/{model} (temp={temperature})")
        
        if provider == "ollama":
            return cls._create_ollama(model, temperature)
        else:
            return cls._create_openai(model, temperature)
    
    # ===================== å…¬å…±æ¥å£ =====================
    
    @classmethod
    def get_light_model(cls) -> BaseChatModel:
        """
        è·å–è½»é‡æ¨¡å‹ - ç”¨äºç®€å•ä»»åŠ¡
        
        é€‚ç”¨åœºæ™¯:
        - ä¿¡æ¯æå–
        - æ ¼å¼è½¬æ¢
        - ç®€å•æ‘˜è¦
        - JSON è§£æ
        
        ç‰¹ç‚¹: é€Ÿåº¦å¿«ã€æˆæœ¬ä½ã€ç¡®å®šæ€§é«˜
        """
        if "light" not in cls._instances:
            cls._instances["light"] = cls._create_instance("light")
        return cls._instances["light"]
    
    @classmethod
    def get_smart_model(cls) -> BaseChatModel:
        """
        è·å–æ™ºèƒ½æ¨¡å‹ - ç”¨äºå¤æ‚ä»»åŠ¡
        
        é€‚ç”¨åœºæ™¯:
        - è¡Œç¨‹è§„åˆ’
        - åˆ›æ„å†™ä½œ
        - å¤æ‚æ¨ç†
        - ä¸ªæ€§åŒ–å»ºè®®
        
        ç‰¹ç‚¹: è´¨é‡é«˜ã€æ›´æœ‰åˆ›æ„
        """
        if "smart" not in cls._instances:
            cls._instances["smart"] = cls._create_instance("smart")
        return cls._instances["smart"]
    
    @classmethod
    def get_default(cls) -> BaseChatModel:
        """è·å–é»˜è®¤æ¨¡å‹ï¼ˆå‘åå…¼å®¹ï¼‰"""
        if "default" not in cls._instances:
            cls._instances["default"] = cls._create_instance("default")
        return cls._instances["default"]
    
    @classmethod
    def get(cls, model_type: str = "default") -> BaseChatModel:
        """
        é€šç”¨è·å–æ–¹æ³•
        
        Args:
            model_type: "light" | "smart" | "default"
        """
        if model_type == "light":
            return cls.get_light_model()
        elif model_type == "smart":
            return cls.get_smart_model()
        else:
            return cls.get_default()
    
    @classmethod
    def clear_cache(cls):
        """æ¸…é™¤ç¼“å­˜çš„å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•æˆ–é‡æ–°åŠ è½½é…ç½®ï¼‰"""
        cls._instances.clear()
        print("ğŸ”„ LLM å®ä¾‹ç¼“å­˜å·²æ¸…é™¤")


# ===================== å‘åå…¼å®¹ =====================

# ä¿æŒåŸæœ‰çš„ Myllm å˜é‡ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹
# Myllm = LLMFactory.get_default()

# ä¾¿æ·åˆ«å
def get_llm(model_type: str = "default") -> BaseChatModel:
    """ä¾¿æ·å‡½æ•°ï¼šè·å– LLM å®ä¾‹"""
    return LLMFactory.get(model_type)


# ===================== æ—§æ¥å£å…¼å®¹ï¼ˆå¯é€‰åˆ é™¤ï¼‰=====================

def create_llm(
    provider: str = None,
    model: str = None,
    temperature: float = 0.7,
    base_url: str = None
) -> BaseChatModel:
    """
    [å·²åºŸå¼ƒ] åˆ›å»º LLM å®ä¾‹ - ä¿ç•™ç”¨äºå‘åå…¼å®¹
    
    æ¨èä½¿ç”¨: LLMFactory.get_light_model() æˆ– LLMFactory.get_smart_model()
    """
    print("âš ï¸ create_llm() å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ LLMFactory")
    
    provider = provider or os.getenv("LLM_PROVIDER", "openai")
    
    if provider.lower() == "ollama":
        model = model or os.getenv("OLLAMA_MODEL", "qwen2:7b")
        base_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        return ChatOllama(
            model=model,
            temperature=temperature,
            base_url=base_url,
            num_ctx=4096,
            timeout=120,
        )
    else:
        model = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        api_key = os.getenv("OPENAI_API_KEY", "sk-dummy-key")
        base_url = base_url or os.getenv("OPENAI_API_BASE")
        return ChatOpenAI(
            model=model,
            temperature=temperature,
            api_key=api_key,
            base_url=base_url,
            timeout=60.0,
        )